apiVersion: jobset.x-k8s.io/v1alpha2
kind: JobSet
metadata:
  #namespace: default
  name: train
  #labels:
  #     kueue.x-k8s.io/queue-name: dws-local-queue
  #annotations:
  #    provreq.kueue.x-k8s.io/maxRunDurationHours: "10"
  #annotations:
  #  kueue.x-k8s.io/queue-name: ${K8S_LOCAL_QUEUE}
spec:
  network:
    enableDNSHostnames: true
  replicatedJobs:
  - name: workers
    template:
      spec:
        parallelism: 2
        completions: 2
        backoffLimit: 0
        #suspend: true #dws
        template:
          metadata:
            annotations:
              #iam.gke.io/gcp-service-account: "${GCP_SERVICE_ACCOUNT}"
              gke-gcsfuse/volumes: "true"
              networking.gke.io/default-interface: "eth0"
              networking.gke.io/interfaces: |
                [
                  {"interfaceName":"eth0","network":"default"},
                  {"interfaceName":"eth1","network":"gvnic-1"},
                  {"interfaceName":"eth2","network":"rdma-0"},
                  {"interfaceName":"eth3","network":"rdma-1"},
                  {"interfaceName":"eth4","network":"rdma-2"},
                  {"interfaceName":"eth5","network":"rdma-3"},
                  {"interfaceName":"eth6","network":"rdma-4"},
                  {"interfaceName":"eth7","network":"rdma-5"},
                  {"interfaceName":"eth8","network":"rdma-6"},
                  {"interfaceName":"eth9","network":"rdma-7"}
                ]
          spec:
            #dnsPolicy: ClusterFirstWithHostNet
            nodeSelector:
              #cloud.google.com/gke-nodepool:  a3-ultragpu-8g-a3-ultragpu-pool #dws
              cloud.google.com/gke-accelerator: nvidia-h200-141gb
            tolerations:
            #- key: cloud.google.com/gke-queued
            #  effect: NoSchedule
            # value: "true"
            - key: nvidia.com/gpu
              operator: Exists
              effect: NoSchedule
            restartPolicy: Never
            serviceAccountName: storage-access
            volumes:
            - name: library-dir-host
              hostPath:
                path: /home/kubernetes/bin/nvidia
            - name: gib
              hostPath:
                path: /home/kubernetes/bin/gib
            - name: data
              emptyDir: {}
            - name: hf-cache
              emptyDir: {}
            - name: model-cache
              emptyDir: {}
            - name: output
              emptyDir: {}
            - name: dshm
              emptyDir:
                medium: Memory
            - name: gcs-fuse-csi-ephemeral
              csi:
                driver: gcsfuse.csi.storage.gke.io
                readOnly: false
                volumeAttributes:
                   bucketName: mdonati-h200-uscentral1
                   mountOptions: "implicit-dirs"
                   fileCacheCapacity: "-1Mi"
                   fileCacheForRangeRead: "true"
                   metadataCacheTTLSeconds: "-1"
                   metadataNegativeCacheTTLSeconds: "0"
                   metadataStatCacheCapacity: "-1Mi"
                   metadataTypeCacheCapacity: "-1Mi"
            - name: local-ssd
              hostPath:
                path: /mnt/stateful_partition/kube-ephemeral-ssd
            containers:
            - name: pytorch
              image: us-central1-docker.pkg.dev/northam-ce-mlai-tpu/mdonati-h200/mlperf:latest
              imagePullPolicy: Always
              # image: gcr.io/k8s-staging-jobset/pytorch-mnist:latest
              ports:
               - containerPort: 3389
              env:
              # - name: HF_TOKEN
              #   valueFrom:
              #     secretKeyRef:
              #       name: huggingface
              #       key: HF_TOKEN
              # - name: HUGGING_FACE_HUB_TOKEN
              #   valueFrom:
              #     secretKeyRef:
              #        name: huggingface
              #        key: HF_TOKEN
              - name: MASTER_ADDR
                value: "train-workers-0-0.train"
              - name: hostname_prefix
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.annotations['jobset.sigs.k8s.io/jobset-name']
              - name: hostname_suffix
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.annotations['jobset.sigs.k8s.io/replicatedjob-name']
              - name: MASTER_PORT
                value: "3389"
              - name: NODE_COUNT
                value: "2"
              - name: NODE_RANK
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']
              # Force python to not buffer output and write directly to stdout, so we can view training logs via `kubectl logs`.
              - name: PYTHONUNBUFFERED
                value: "1"
              - name: LD_LIBRARY_PATH
                value: /usr/local/nvidia/lib64
              - name: "TORCH_DISTRIBUTED_BACKEND"
                value: "nccl"
              securityContext:
                privileged: true
              command: ["/bin/bash", "-c"]
              args:
              - |
                set -e
                ldconfig $LD_LIBRARY_PATH
                #echo "Added $LD_LIBRARY_PATH to ldconfig:"
                ldconfig -p | grep libcuda | sed 's/^/  /'
                echo ""
                #sleep infinity
                #TORCH_DISTRIBUTED_DEBUG=DETAIL OMP_NUM_THREADS=2 accelerate launch --config_file configs/default_config.yaml  --gpu_ids all --num_machines=$NODE_COUNT --num_processes=16 --rdzv_backend static --main_process_ip=$MASTER_ADDR --main_process_port=$MASTER_PORT --machine_rank=$NODE_RANK --debug  \
                cp -f /gcs-dir/training.sh scripts/training.sh
                cp -f /gcs-dir/train.py scripts/train.py
                bash scripts/training.sh "\${MASTER_ADDR}" "\${MASTER_PORT}" "\${NODE_RANK}" "\${NODE_COUNT}" "/gcs-dir/meta-llama--Llama-2-70B-hf"
                #bash /gcs-dir/training.sh "\${MASTER_ADDR}" "\${MASTER_PORT}" "\${NODE_RANK}" "\${NODE_COUNT}" "/gcs-dir/meta-llama--Llama-2-70B-hf"
              resources:  
                requests:
                  nvidia.com/gpu: 8
                limits:
                  nvidia.com/gpu: 8
              volumeMounts:
               - name: library-dir-host
                 mountPath: /usr/local/nvidia
               - name: gib
                 mountPath: /usr/local/gib
               - name: dshm
                 mountPath: /dev/shm
               - name: data
                 mountPath: /data
               - name: hf-cache
                 mountPath: /root/.cache/huggingface
               - name: model-cache
                 mountPath: /root/.cache/modelscope
               - name: output
                 mountPath: /app/output
               - mountPath: /gcs-dir
                 name: gcs-fuse-csi-ephemeral
               - name: local-ssd
                 mountPath: /localssd
               
           