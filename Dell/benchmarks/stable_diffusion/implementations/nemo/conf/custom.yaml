# Copyright (c) 2023-2025, NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Please generate encoded captions with `encode_captions.sh` first.
name: null

trainer:
  devices: null
  num_nodes: null
  accelerator: gpu
  logger: False # logger provided by exp_manager
  enable_checkpointing: False
#  replace_sampler_ddp: False  # Removed in newer Nemo MM
  max_epochs: -1 # PTL default. In practice, max_steps will be reached first.
  max_steps: null # consumed_samples = global_step * micro_batch_size * data_parallel_size * accumulate_grad_batches
  log_every_n_steps: 10000
  accumulate_grad_batches: 1 # do not modify, grad acc is automatic for training megatron models
  benchmark: False
  enable_model_summary: True

exp_manager:
  exp_dir: null
  name: ${name}
  create_wandb_logger: False
  log_tflops_per_sec_per_gpu: False
  wandb_logger_kwargs:
    project: stable-diffusion
    group: nemo-sd
    name: ${name}
    resume: True
  create_checkpoint_callback: True
  create_tensorboard_logger: True
  checkpoint_callback_params:
    every_n_train_steps: null
    every_n_epochs: 0
    monitor: timestamp
    filename: '${name}--{timestamp}-{step}-{consumed_samples}'
    save_top_k: -1
    save_last: False
    save_nemo_on_train_end: False
    save_weights_only: True
  resume_if_exists: True
  resume_ignore_no_checkpoint: True
  ema:
    enable: False
    decay: 0.9999
    validate_original_weights: False
    every_n_steps: 1
    cpu_offload: False
  create_preemption_callback: False
  log_step_timing: False  # leads to sync and not allowed during CUDA graph capturing


model:
  precision: 16 # 16-mixed for AMP O2, 16 for full fp16
  # specify micro_batch_size, global_batch_size, and model parallelism
  # gradient accumulation will be done automatically based on data_parallel_size
  micro_batch_size: null # limited by GPU memory
  global_batch_size: null # will use more micro batches to reach global batch size

  linear_start: 0.00085
  linear_end: 0.012
  num_timesteps_cond: 1
  log_every_t: 200
  timesteps: 1000
  first_stage_key: images_moments # images | images_encoded | images_moments
  cond_stage_key: clip_encoded # txt for cifar | caption for pbss | captions_encoded for clip-embed offline
  image_size: 64
  channels: 4
  cond_stage_trainable: false
  conditioning_key: crossattn # check
  monitor: val/loss_simple_ema
  scale_factor: 0.18215
  use_ema: False
  scale_by_std: False

  ckpt_path: /checkpoints/sd/512-base-ema.ckpt
  load_vae: True
  load_unet: False
  load_encoder: True

  ignore_keys: []
  parameterization: v
  clip_denoised: True
  load_only_unet: False
  cosine_s: 8e-3
  given_betas:
  original_elbo_weight: 0
  v_posterior: 0
  l_simple_weight: 1
  use_positional_encodings: False
  learn_logvar: False
  logvar_init: 0
  beta_schedule: linear
  loss_type: l2
  channels_last: True

  concat_mode: True
  cond_stage_forward:
  text_embedding_dropout_rate: 0.0
  fused_opt: True
  inductor: True
  inductor_cudagraphs: False
  capture_cudagraph_iters: 15 # -1 to disable
  async_checkpoint_io: True
  use_cudnn_layer_norm: False
  use_torch_sched: False
  #megatron_amp_O2: True
  #autocast_dtype: torch.float16

  unet_config:
    _target_: nemo.collections.multimodal.modules.stable_diffusion.diffusionmodules.openaimodel.UNetModel
    from_pretrained:
    from_NeMo: #Must be specified when from pretrained is not None, False means loading unet from HF ckpt
    image_size: 32 # unused
    in_channels: 4
    out_channels: 4
    model_channels: 320
    attention_resolutions:
    - 4
    - 2
    - 1
    num_res_blocks: 2
    channel_mult:
    - 1
    - 2
    - 4
    - 4
    num_head_channels: 64
    use_spatial_transformer: true
    use_linear_in_transformer: true
    transformer_depth: 1
    context_dim: 1024
    use_checkpoint: False
    legacy: False
    use_flash_attention: null
    use_te_dpa: null
    resblock_gn_groups: 16
    unet_precision: fp16  # fp16-mixed for AMP O2, fp16 for AMP O2
    timesteps: ${model.timesteps}

  first_stage_config:
    _target_: nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.autoencoder.AutoencoderKL
    from_pretrained:
    embed_dim: 4
    monitor: val/rec_loss
    ddconfig:
      double_z: true
      z_channels: 4
      resolution: 256  #Never used
      in_channels: 3
      out_ch: 3
      ch: 128
      ch_mult:
      - 1
      - 2
      - 4
      - 4
      num_res_blocks: 2
      attn_resolutions: []
      dropout: 0.0
    lossconfig:
      target: torch.nn.Identity

  cond_stage_config:
#    _target_: nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.FrozenMegatronCLIPEmbedder
#    restore_from_path: /checkpoints/clip/open_clip_pytorch_model.bin
#    device: cuda
#    freeze: True
#    layer: "penultimate"
    _target_: nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.FrozenOpenCLIPEmbedder
    arch: "ViT-H-14"
    version: "laion2b_s32b_b79k"
    freeze: True
    layer: "penultimate"
    cache_dir: /checkpoints/clip

  # miscellaneous
  seed: 1234
  resume_from_checkpoint: null # manually set the checkpoint file to load from
  apex_transformer_log_level: 30 # Python logging level displays logs with severity greater than or equal to this
  gradient_as_bucket_view: True # PyTorch DDP argument. Allocate gradients in a contiguous bucket to save memory (less fragmentation and buffer memory)
  ddp_overlap: False # True for using PyTorch DDP overlap.

  # Nsys profiling options
  nsys_profile:
    enabled: False
    start_step: 10  # Global batch to start profiling
    end_step: 10 # Global batch to end profiling
    ranks: [ 0 ] # Global rank IDs to profile
    gen_shape: False # Generate model and kernel details including input shapes

  data:
      num_workers: 16
      train:
          dataset_path: /datasets/*.tar
          augmentations:
            resize_smallest_side: 512
            center_crop_h_w: 512, 512
            horizontal_flip: False
          filterings:

      webdataset:
          infinite_sampler: True
          local_root_path: /datasets
