diff --git a/nemo/collections/multimodal/modules/stable_diffusion/attention.py b/nemo/collections/multimodal/modules/stable_diffusion/attention.py
index c92980d90..638261ccf 100644
--- a/nemo/collections/multimodal/modules/stable_diffusion/attention.py
+++ b/nemo/collections/multimodal/modules/stable_diffusion/attention.py
@@ -16,7 +16,6 @@ from inspect import isfunction
 
 import torch
 import torch.nn.functional as F
-from apex.contrib.group_norm import GroupNorm
 from einops import rearrange, repeat
 from torch import einsum, nn
 from torch._dynamo import disable
@@ -118,7 +117,13 @@ def zero_module(module):
 
 
 def Normalize(in_channels, num_groups=32, act=""):
-    return GroupNorm(num_groups=num_groups, num_channels=in_channels, eps=1e-6, affine=True, act=act)
+    try:
+        from apex.contrib.group_norm import GroupNorm as GroupNorm
+        return GroupNorm(num_groups=num_groups, num_channels=in_channels, eps=1e-6, affine=True, act=act)
+    except ImportError:
+        print("Using torch.nn.GroupNorm. Hip/Cuda kernel could not be imported from Apex")
+        import torch.nn.GroupNorm as GroupNorm
+        return GroupNorm(num_groups=num_groups, num_channels=in_channels, eps=1e-6, affine=True)
 
 
 class LinearAttention(nn.Module):
@@ -507,4 +512,4 @@ class SpatialTransformer(nn.Module):
         x = x.transpose(1, 2).view(b, c, h, w)  # b (h w) c -> b c h w
         if not self.use_linear:
             x = self.proj_out(x)
-        return x_in + x
+        return x_in + x
\ No newline at end of file
