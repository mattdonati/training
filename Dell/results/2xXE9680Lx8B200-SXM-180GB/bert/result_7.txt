+ echo 'Beginning trial 08 of 10'
Beginning trial 08 of 10
+ echo ':::DLPAL dockerd://mlperf-dell:bert 156 2 tnasnt[6-7] '\''unknown'\'' XE9680Lx8B200-SXM-180GB_2x8x48x1_pack'
:::DLPAL dockerd://mlperf-dell:bert 156 2 tnasnt[6-7] 'unknown' XE9680Lx8B200-SXM-180GB_2x8x48x1_pack
++ srun -N1 -n1 --container-name=language_model_156 --no-container-mount-home --container-remap-root --container-writable mlperf-sysjson.sh
+ echo ':::SYSJSON {"submitter":"UNKNOWN_MLPERF_SUBMITTER","division":"closed","status":"Available on-premise","system_name":"UNKNOWN_MLPERF_SYSTEM_NAME","number_of_nodes":"2","host_processors_per_node":"2","host_processor_model_name":"INTEL(R) XEON(R) PLATINUM 8562Y+","host_processor_core_count":"32","host_processor_vcpu_count":"","host_processor_frequency":"","host_processor_caches":"","host_processor_interconnect":"","host_memory_capacity":"3.9 TB","host_storage_type":"","host_storage_capacity":"","host_networking":"","host_networking_topology":"","host_memory_configuration":"","accelerators_per_node":"8","accelerator_model_name":"Dell B200","accelerator_host_interconnect":"","accelerator_frequency":"","accelerator_on-chip_memories":"","accelerator_memory_configuration":"","accelerator_memory_capacity":"183359 MiB","accelerator_interconnect":"","accelerator_interconnect_topology":"","cooling":"","hw_notes":"","framework":"PyTorch Dell Release 25.04","framework_name":"","other_software_stack":{"cuda_version":"12.9.0.036","cuda_driver_version":"575.51.02","nccl_version":"2.26.3","cublas_version":"12.9.0.2","cudnn_version":"9.9.0.52","trt_version":"10.9.0.34+cuda12.8","dali_version":"1.48.0","mofed_version":"5.4-rdmacore50.0","openmpi_version":"4.1.7","kernel_version":"Linux 5.15.0-136-generic","nvidia_kernel_driver":"570.124.06"},"operating_system":"Ubuntu 24.04.2 LTS","sw_notes":""}'
:::SYSJSON {"submitter":"UNKNOWN_MLPERF_SUBMITTER","division":"closed","status":"Available on-premise","system_name":"UNKNOWN_MLPERF_SYSTEM_NAME","number_of_nodes":"2","host_processors_per_node":"2","host_processor_model_name":"INTEL(R) XEON(R) PLATINUM 8562Y+","host_processor_core_count":"32","host_processor_vcpu_count":"","host_processor_frequency":"","host_processor_caches":"","host_processor_interconnect":"","host_memory_capacity":"3.9 TB","host_storage_type":"","host_storage_capacity":"","host_networking":"","host_networking_topology":"","host_memory_configuration":"","accelerators_per_node":"8","accelerator_model_name":"Dell B200","accelerator_host_interconnect":"","accelerator_frequency":"","accelerator_on-chip_memories":"","accelerator_memory_configuration":"","accelerator_memory_capacity":"183359 MiB","accelerator_interconnect":"","accelerator_interconnect_topology":"","cooling":"","hw_notes":"","framework":"PyTorch Dell Release 25.04","framework_name":"","other_software_stack":{"cuda_version":"12.9.0.036","cuda_driver_version":"575.51.02","nccl_version":"2.26.3","cublas_version":"12.9.0.2","cudnn_version":"9.9.0.52","trt_version":"10.9.0.34+cuda12.8","dali_version":"1.48.0","mofed_version":"5.4-rdmacore50.0","openmpi_version":"4.1.7","kernel_version":"Linux 5.15.0-136-generic","nvidia_kernel_driver":"570.124.06"},"operating_system":"Ubuntu 24.04.2 LTS","sw_notes":""}
+ srun -N1 -n1 --container-name=language_model_156 --no-container-mount-home --container-remap-root --container-writable bash -c 'echo ":::GITCOMMITID ${GIT_COMMIT_ID} ${LAUNCHER_GIT_COMMIT_ID}"'
:::GITCOMMITID  
+ '[' 1 -eq 1 ']'
+ srun --ntasks-per-node=1 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
Clearing cache on tnasnt7
Clearing cache on tnasnt6
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks-per-node=1 --container-name=language_model_156 --no-container-mount-home --container-remap-root --container-writable python -c '
from mlperf_logger import mllogger
mllogger.event(key=mllogger.constants.CACHE_CLEAR, value=True)'
:::MLLOG {"namespace": "", "time_ms": 1746134414994, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1746134413027, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
+ set +e
++ date +%s
+ echo 'RUNANDTIME_START 1746134413'
RUNANDTIME_START 1746134413
+ srun -l --mpi=none --ntasks-per-node=8 --time=12 --container-name=language_model_156 --no-container-mount-home --container-remap-root --container-writable --container-mounts=/training_datasets_v5.0/training_datasets_v4.1/bert/packed_data:/workspace/data_phase2,/training_datasets_v5.0/training_datasets_v4.1/bert/phase1:/workspace/phase1,/training_datasets_v5.0/training_datasets_v4.1/bert/hdf5/eval_varlength:/workspace/evaldata,/root/training_results_v5.0/bert/tunas_singlenode_results:/results --container-workdir=/workspace/bert --container-env=MASTER_PORT,MASTER_ADDR slurm2pytorch ./run_and_time.sh
12: Run vars: id 156 gpus 8 mparams ''
13: Run vars: id 156 gpus 8 mparams ''
14: Run vars: id 156 gpus 8 mparams ''
 9: Run vars: id 156 gpus 8 mparams ''
10: Run vars: id 156 gpus 8 mparams ''
15: Run vars: id 156 gpus 8 mparams ''
11: Run vars: id 156 gpus 8 mparams ''
 8: Run vars: id 156 gpus 8 mparams ''
 7: Run vars: id 156 gpus 8 mparams ''
 0: Run vars: id 156 gpus 8 mparams ''
 3: Run vars: id 156 gpus 8 mparams ''
 6: Run vars: id 156 gpus 8 mparams ''
 5: Run vars: id 156 gpus 8 mparams ''
 2: Run vars: id 156 gpus 8 mparams ''
 4: Run vars: id 156 gpus 8 mparams ''
 1: Run vars: id 156 gpus 8 mparams ''
 8: [W501 21:20:19.252626737 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 9: [W501 21:20:19.252622923 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
10: [W501 21:20:19.252622886 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
11: [W501 21:20:19.252622873 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
12: [W501 21:20:19.252626036 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
13: [W501 21:20:19.252626653 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
14: [W501 21:20:19.252622812 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
15: [W501 21:20:19.252628990 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 8: /usr/local/lib/python3.12/dist-packages/apex/__init__.py:67: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 8:   warnings.warn(msg, DeprecatedFeatureWarning)
 9: /usr/local/lib/python3.12/dist-packages/apex/__init__.py:67: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 9:   warnings.warn(msg, DeprecatedFeatureWarning)
10: /usr/local/lib/python3.12/dist-packages/apex/__init__.py:67: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
10:   warnings.warn(msg, DeprecatedFeatureWarning)
11: /usr/local/lib/python3.12/dist-packages/apex/__init__.py:67: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
11:   warnings.warn(msg, DeprecatedFeatureWarning)
12: /usr/local/lib/python3.12/dist-packages/apex/__init__.py:67: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
12:   warnings.warn(msg, DeprecatedFeatureWarning)
13: /usr/local/lib/python3.12/dist-packages/apex/__init__.py:67: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
13:   warnings.warn(msg, DeprecatedFeatureWarning)
14: /usr/local/lib/python3.12/dist-packages/apex/__init__.py:67: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
14:   warnings.warn(msg, DeprecatedFeatureWarning)
15: /usr/local/lib/python3.12/dist-packages/apex/__init__.py:67: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
15:   warnings.warn(msg, DeprecatedFeatureWarning)
 0: [W501 21:20:17.116388681 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 1: [W501 21:20:17.116385248 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 2: [W501 21:20:17.116393150 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 3: [W501 21:20:17.116388060 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 4: [W501 21:20:17.116386236 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 5: [W501 21:20:17.116391710 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 6: [W501 21:20:17.116386184 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 7: [W501 21:20:17.116391765 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: /usr/local/lib/python3.12/dist-packages/apex/__init__.py:67: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 0:   warnings.warn(msg, DeprecatedFeatureWarning)
 1: /usr/local/lib/python3.12/dist-packages/apex/__init__.py:67: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 1:   warnings.warn(msg, DeprecatedFeatureWarning)
 6: /usr/local/lib/python3.12/dist-packages/apex/__init__.py:67: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 6:   warnings.warn(msg, DeprecatedFeatureWarning)
 2: /usr/local/lib/python3.12/dist-packages/apex/__init__.py:67: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 2:   warnings.warn(msg, DeprecatedFeatureWarning)
 3: /usr/local/lib/python3.12/dist-packages/apex/__init__.py:67: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 3:   warnings.warn(msg, DeprecatedFeatureWarning)
 4: /usr/local/lib/python3.12/dist-packages/apex/__init__.py:67: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 4:   warnings.warn(msg, DeprecatedFeatureWarning)
 5: /usr/local/lib/python3.12/dist-packages/apex/__init__.py:67: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 5:   warnings.warn(msg, DeprecatedFeatureWarning)
 7: /usr/local/lib/python3.12/dist-packages/apex/__init__.py:67: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 7:   warnings.warn(msg, DeprecatedFeatureWarning)
 6: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:56: UserWarning: transformer_engine failed to import with exception cannot import name 'MXFP8BlockScaling' from 'transformer_engine.common.recipe' (/usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py)
 6:   warnings.warn(f"transformer_engine failed to import with exception {ex}")
 6: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:62: UserWarning: Installed version of transformer_engine 1.11.0.dev0+98aa27a is not supported, please upgrade to version 2.0 from https://github.com/Dell/TransformerEngine/tree/release_v2.0. `transformer_engine_ex` will not be used.
 6:   warnings.warn(msg)
12: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:56: UserWarning: transformer_engine failed to import with exception cannot import name 'MXFP8BlockScaling' from 'transformer_engine.common.recipe' (/usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py)
12:   warnings.warn(f"transformer_engine failed to import with exception {ex}")
12: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:62: UserWarning: Installed version of transformer_engine 1.11.0.dev0+98aa27a is not supported, please upgrade to version 2.0 from https://github.com/Dell/TransformerEngine/tree/release_v2.0. `transformer_engine_ex` will not be used.
12:   warnings.warn(msg)
 6: [W501 21:20:29.256938017 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 6: /workspace/bert/run_pretraining.py:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
 6:   grad_scaler = GradScaler(init_scale=float(os.getenv("INIT_LOSS_SCALE", 2**20)), growth_interval=2000)
 6: :::MLLOG {"namespace": "", "time_ms": 1746134429750, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1390}}
12: [W501 21:20:32.111976272 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
12: /workspace/bert/run_pretraining.py:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
12:   grad_scaler = GradScaler(init_scale=float(os.getenv("INIT_LOSS_SCALE", 2**20)), growth_interval=2000)
12: :::MLLOG {"namespace": "", "time_ms": 1746134432552, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1390}}
14: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:56: UserWarning: transformer_engine failed to import with exception cannot import name 'MXFP8BlockScaling' from 'transformer_engine.common.recipe' (/usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py)
14:   warnings.warn(f"transformer_engine failed to import with exception {ex}")
14: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:62: UserWarning: Installed version of transformer_engine 1.11.0.dev0+98aa27a is not supported, please upgrade to version 2.0 from https://github.com/Dell/TransformerEngine/tree/release_v2.0. `transformer_engine_ex` will not be used.
14:   warnings.warn(msg)
14: [W501 21:20:32.371602243 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
14: /workspace/bert/run_pretraining.py:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
14:   grad_scaler = GradScaler(init_scale=float(os.getenv("INIT_LOSS_SCALE", 2**20)), growth_interval=2000)
14: :::MLLOG {"namespace": "", "time_ms": 1746134432811, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1390}}
15: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:56: UserWarning: transformer_engine failed to import with exception cannot import name 'MXFP8BlockScaling' from 'transformer_engine.common.recipe' (/usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py)
15:   warnings.warn(f"transformer_engine failed to import with exception {ex}")
10: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:56: UserWarning: transformer_engine failed to import with exception cannot import name 'MXFP8BlockScaling' from 'transformer_engine.common.recipe' (/usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py)
10:   warnings.warn(f"transformer_engine failed to import with exception {ex}")
15: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:62: UserWarning: Installed version of transformer_engine 1.11.0.dev0+98aa27a is not supported, please upgrade to version 2.0 from https://github.com/Dell/TransformerEngine/tree/release_v2.0. `transformer_engine_ex` will not be used.
15:   warnings.warn(msg)
10: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:62: UserWarning: Installed version of transformer_engine 1.11.0.dev0+98aa27a is not supported, please upgrade to version 2.0 from https://github.com/Dell/TransformerEngine/tree/release_v2.0. `transformer_engine_ex` will not be used.
10:   warnings.warn(msg)
15: [W501 21:20:32.448868444 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
10: [W501 21:20:32.449693400 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
15: /workspace/bert/run_pretraining.py:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
15:   grad_scaler = GradScaler(init_scale=float(os.getenv("INIT_LOSS_SCALE", 2**20)), growth_interval=2000)
10: /workspace/bert/run_pretraining.py:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
10:   grad_scaler = GradScaler(init_scale=float(os.getenv("INIT_LOSS_SCALE", 2**20)), growth_interval=2000)
15: :::MLLOG {"namespace": "", "time_ms": 1746134432888, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1390}}
10: :::MLLOG {"namespace": "", "time_ms": 1746134432889, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1390}}
 8: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:56: UserWarning: transformer_engine failed to import with exception cannot import name 'MXFP8BlockScaling' from 'transformer_engine.common.recipe' (/usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py)
 8:   warnings.warn(f"transformer_engine failed to import with exception {ex}")
 8: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:62: UserWarning: Installed version of transformer_engine 1.11.0.dev0+98aa27a is not supported, please upgrade to version 2.0 from https://github.com/Dell/TransformerEngine/tree/release_v2.0. `transformer_engine_ex` will not be used.
 8:   warnings.warn(msg)
 8: [W501 21:20:32.461618099 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 8: /workspace/bert/run_pretraining.py:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
 8:   grad_scaler = GradScaler(init_scale=float(os.getenv("INIT_LOSS_SCALE", 2**20)), growth_interval=2000)
 8: :::MLLOG {"namespace": "", "time_ms": 1746134432901, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1390}}
 9: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:56: UserWarning: transformer_engine failed to import with exception cannot import name 'MXFP8BlockScaling' from 'transformer_engine.common.recipe' (/usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py)
 9:   warnings.warn(f"transformer_engine failed to import with exception {ex}")
13: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:56: UserWarning: transformer_engine failed to import with exception cannot import name 'MXFP8BlockScaling' from 'transformer_engine.common.recipe' (/usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py)
13:   warnings.warn(f"transformer_engine failed to import with exception {ex}")
 9: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:62: UserWarning: Installed version of transformer_engine 1.11.0.dev0+98aa27a is not supported, please upgrade to version 2.0 from https://github.com/Dell/TransformerEngine/tree/release_v2.0. `transformer_engine_ex` will not be used.
 9:   warnings.warn(msg)
13: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:62: UserWarning: Installed version of transformer_engine 1.11.0.dev0+98aa27a is not supported, please upgrade to version 2.0 from https://github.com/Dell/TransformerEngine/tree/release_v2.0. `transformer_engine_ex` will not be used.
13:   warnings.warn(msg)
11: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:56: UserWarning: transformer_engine failed to import with exception cannot import name 'MXFP8BlockScaling' from 'transformer_engine.common.recipe' (/usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py)
11:   warnings.warn(f"transformer_engine failed to import with exception {ex}")
11: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:62: UserWarning: Installed version of transformer_engine 1.11.0.dev0+98aa27a is not supported, please upgrade to version 2.0 from https://github.com/Dell/TransformerEngine/tree/release_v2.0. `transformer_engine_ex` will not be used.
11:   warnings.warn(msg)
 9: [W501 21:20:32.475385122 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
13: [W501 21:20:32.475906923 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
11: [W501 21:20:32.477025828 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 9: /workspace/bert/run_pretraining.py:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
 9:   grad_scaler = GradScaler(init_scale=float(os.getenv("INIT_LOSS_SCALE", 2**20)), growth_interval=2000)
13: /workspace/bert/run_pretraining.py:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
13:   grad_scaler = GradScaler(init_scale=float(os.getenv("INIT_LOSS_SCALE", 2**20)), growth_interval=2000)
11: /workspace/bert/run_pretraining.py:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
11:   grad_scaler = GradScaler(init_scale=float(os.getenv("INIT_LOSS_SCALE", 2**20)), growth_interval=2000)
 9: :::MLLOG {"namespace": "", "time_ms": 1746134432915, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1390}}
13: :::MLLOG {"namespace": "", "time_ms": 1746134432915, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1390}}
11: :::MLLOG {"namespace": "", "time_ms": 1746134432917, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1390}}
 3: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:56: UserWarning: transformer_engine failed to import with exception cannot import name 'MXFP8BlockScaling' from 'transformer_engine.common.recipe' (/usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py)
 3:   warnings.warn(f"transformer_engine failed to import with exception {ex}")
 3: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:62: UserWarning: Installed version of transformer_engine 1.11.0.dev0+98aa27a is not supported, please upgrade to version 2.0 from https://github.com/Dell/TransformerEngine/tree/release_v2.0. `transformer_engine_ex` will not be used.
 3:   warnings.warn(msg)
 3: [W501 21:20:30.862361439 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 3: /workspace/bert/run_pretraining.py:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
 3:   grad_scaler = GradScaler(init_scale=float(os.getenv("INIT_LOSS_SCALE", 2**20)), growth_interval=2000)
 3: :::MLLOG {"namespace": "", "time_ms": 1746134430354, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1390}}
 7: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:56: UserWarning: transformer_engine failed to import with exception cannot import name 'MXFP8BlockScaling' from 'transformer_engine.common.recipe' (/usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py)
 7:   warnings.warn(f"transformer_engine failed to import with exception {ex}")
 7: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:62: UserWarning: Installed version of transformer_engine 1.11.0.dev0+98aa27a is not supported, please upgrade to version 2.0 from https://github.com/Dell/TransformerEngine/tree/release_v2.0. `transformer_engine_ex` will not be used.
 7:   warnings.warn(msg)
 0: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:56: UserWarning: transformer_engine failed to import with exception cannot import name 'MXFP8BlockScaling' from 'transformer_engine.common.recipe' (/usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py)
 0:   warnings.warn(f"transformer_engine failed to import with exception {ex}")
 0: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:62: UserWarning: Installed version of transformer_engine 1.11.0.dev0+98aa27a is not supported, please upgrade to version 2.0 from https://github.com/Dell/TransformerEngine/tree/release_v2.0. `transformer_engine_ex` will not be used.
 0:   warnings.warn(msg)
 7: [W501 21:20:30.885723281 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 0: [W501 21:20:30.887197851 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 7: /workspace/bert/run_pretraining.py:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
 7:   grad_scaler = GradScaler(init_scale=float(os.getenv("INIT_LOSS_SCALE", 2**20)), growth_interval=2000)
 0: /workspace/bert/run_pretraining.py:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
 0:   grad_scaler = GradScaler(init_scale=float(os.getenv("INIT_LOSS_SCALE", 2**20)), growth_interval=2000)
 7: :::MLLOG {"namespace": "", "time_ms": 1746134430378, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1390}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134430379, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1390}}
 2: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:56: UserWarning: transformer_engine failed to import with exception cannot import name 'MXFP8BlockScaling' from 'transformer_engine.common.recipe' (/usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py)
 2:   warnings.warn(f"transformer_engine failed to import with exception {ex}")
 5: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:56: UserWarning: transformer_engine failed to import with exception cannot import name 'MXFP8BlockScaling' from 'transformer_engine.common.recipe' (/usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py)
 5:   warnings.warn(f"transformer_engine failed to import with exception {ex}")
 2: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:62: UserWarning: Installed version of transformer_engine 1.11.0.dev0+98aa27a is not supported, please upgrade to version 2.0 from https://github.com/Dell/TransformerEngine/tree/release_v2.0. `transformer_engine_ex` will not be used.
 2:   warnings.warn(msg)
 5: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:62: UserWarning: Installed version of transformer_engine 1.11.0.dev0+98aa27a is not supported, please upgrade to version 2.0 from https://github.com/Dell/TransformerEngine/tree/release_v2.0. `transformer_engine_ex` will not be used.
 5:   warnings.warn(msg)
 2: [W501 21:20:30.056530307 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 5: [W501 21:20:30.056609307 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 2: /workspace/bert/run_pretraining.py:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
 2:   grad_scaler = GradScaler(init_scale=float(os.getenv("INIT_LOSS_SCALE", 2**20)), growth_interval=2000)
 5: /workspace/bert/run_pretraining.py:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
 5:   grad_scaler = GradScaler(init_scale=float(os.getenv("INIT_LOSS_SCALE", 2**20)), growth_interval=2000)
 2: :::MLLOG {"namespace": "", "time_ms": 1746134430549, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1390}}
 5: :::MLLOG {"namespace": "", "time_ms": 1746134430549, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1390}}
 8: [W501 21:20:33.005263600 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
 8: device: cuda:0 n_gpu: 16, distributed training: True, 16-bits training: True
 4: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:56: UserWarning: transformer_engine failed to import with exception cannot import name 'MXFP8BlockScaling' from 'transformer_engine.common.recipe' (/usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py)
 4:   warnings.warn(f"transformer_engine failed to import with exception {ex}")
 4: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:62: UserWarning: Installed version of transformer_engine 1.11.0.dev0+98aa27a is not supported, please upgrade to version 2.0 from https://github.com/Dell/TransformerEngine/tree/release_v2.0. `transformer_engine_ex` will not be used.
 4:   warnings.warn(msg)
 4: [W501 21:20:30.167125005 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 4: /workspace/bert/run_pretraining.py:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
 4:   grad_scaler = GradScaler(init_scale=float(os.getenv("INIT_LOSS_SCALE", 2**20)), growth_interval=2000)
 4: :::MLLOG {"namespace": "", "time_ms": 1746134430659, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1390}}
 1: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:56: UserWarning: transformer_engine failed to import with exception cannot import name 'MXFP8BlockScaling' from 'transformer_engine.common.recipe' (/usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py)
 1:   warnings.warn(f"transformer_engine failed to import with exception {ex}")
 1: /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg/thunder/executors/transformer_engineex.py:62: UserWarning: Installed version of transformer_engine 1.11.0.dev0+98aa27a is not supported, please upgrade to version 2.0 from https://github.com/Dell/TransformerEngine/tree/release_v2.0. `transformer_engine_ex` will not be used.
 1:   warnings.warn(msg)
 1: [W501 21:20:30.201867742 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
 1: /workspace/bert/run_pretraining.py:83: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
 1:   grad_scaler = GradScaler(init_scale=float(os.getenv("INIT_LOSS_SCALE", 2**20)), growth_interval=2000)
 1: :::MLLOG {"namespace": "", "time_ms": 1746134430694, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1390}}
 8: Torch distributed is available.
 8: Torch distributed is initialized.
11: [W501 21:20:35.499474066 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
11: device: cuda:3 n_gpu: 16, distributed training: True, 16-bits training: True
13: [W501 21:20:35.500533648 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
13: device: cuda:5 n_gpu: 16, distributed training: True, 16-bits training: True
14: [W501 21:20:35.503335450 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
14: device: cuda:6 n_gpu: 16, distributed training: True, 16-bits training: True
11: Torch distributed is available.
11: Torch distributed is initialized.
13: Torch distributed is available.
13: Torch distributed is initialized.
 9: [W501 21:20:35.507652343 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
 9: device: cuda:1 n_gpu: 16, distributed training: True, 16-bits training: True
14: Torch distributed is available.
14: Torch distributed is initialized.
10: [W501 21:20:35.508893174 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
10: device: cuda:2 n_gpu: 16, distributed training: True, 16-bits training: True
 9: Torch distributed is available.
 9: Torch distributed is initialized.
10: Torch distributed is available.
10: Torch distributed is initialized.
15: [W501 21:20:35.514520659 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
15: device: cuda:7 n_gpu: 16, distributed training: True, 16-bits training: True
15: Torch distributed is available.
15: Torch distributed is initialized.
12: [W501 21:20:35.518682357 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
12: device: cuda:4 n_gpu: 16, distributed training: True, 16-bits training: True
12: Torch distributed is available.
12: Torch distributed is initialized.
 6: [W501 21:20:33.018826027 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
 6: device: cuda:6 n_gpu: 16, distributed training: True, 16-bits training: True
 6: Torch distributed is available.
 6: Torch distributed is initialized.
 4: [W501 21:20:33.059046787 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
 4: device: cuda:4 n_gpu: 16, distributed training: True, 16-bits training: True
 4: Torch distributed is available.
 4: Torch distributed is initialized.
 5: [W501 21:20:33.085429527 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
 5: device: cuda:5 n_gpu: 16, distributed training: True, 16-bits training: True
 7: [W501 21:20:33.086320059 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
 7: device: cuda:7 n_gpu: 16, distributed training: True, 16-bits training: True
 3: [W501 21:20:33.086647577 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
 3: device: cuda:3 n_gpu: 16, distributed training: True, 16-bits training: True
 2: [W501 21:20:33.088058152 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
 1: [W501 21:20:33.088489669 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
 2: device: cuda:2 n_gpu: 16, distributed training: True, 16-bits training: True
 1: device: cuda:1 n_gpu: 16, distributed training: True, 16-bits training: True
 5: Torch distributed is available.
 5: Torch distributed is initialized.
 7: Torch distributed is available.
 7: Torch distributed is initialized.
 3: Torch distributed is available.
 3: Torch distributed is initialized.
 0: [W501 21:20:33.096091366 Utils.hpp:137] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
 2: Torch distributed is available.
 2: Torch distributed is initialized.
 0: device: cuda:0 n_gpu: 16, distributed training: True, 16-bits training: True
 0: :::MLLOG {"namespace": "", "time_ms": 1746134433585, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "bert", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1397}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134433585, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Dell", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1397}}
 1: Torch distributed is available.
 0: :::MLLOG {"namespace": "", "time_ms": 1746134433585, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1397}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134433585, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1397}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134433585, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "2xXE9680Lx8B200-SXM-180GB", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1397}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134433585, "event_type": "POINT_IN_TIME", "key": "seed", "value": 30726, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1399}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134433585, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 1536, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1401}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134433585, "event_type": "POINT_IN_TIME", "key": "d_batch_size", "value": 48, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1403}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134433585, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1405}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134433585, "event_type": "POINT_IN_TIME", "key": "max_predictions_per_seq", "value": 76, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1407}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134433586, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_training_steps", "value": 3680.0, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1409}}
 1: Torch distributed is initialized.
 0: :::MLLOG {"namespace": "", "time_ms": 1746134433586, "event_type": "POINT_IN_TIME", "key": "num_warmup_steps", "value": 0, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1411}}
 0: parsed args:
 0: Namespace(input_dir='/workspace/data_phase2', packed_samples=True, order_samples=False, max_pack_factor=3, average_packing_rate=2, synthetic_input=False, bert_model='bert-large-uncased', cuda_graph_mode='segmented', max_iterations_per_graph=4, output_dir='/results', eval_dir='/workspace/evaldata', eval_iter_start_samples=150000, eval_iter_samples=150000, num_eval_examples=10000, cache_eval_data=True, load_eval_synchronously=False, init_checkpoint='/workspace/phase1/model.ckpt-28252.pt', init_tf_checkpoint=None, max_seq_length=512, max_predictions_per_seq=76, train_batch_size=48, eval_batch_size=16, learning_rate=0.00096, weight_decay_rate=0.1, opt_lamb_beta_1=0.60466, opt_lamb_beta_2=0.85437, max_steps=3680.0, sustained_training_time=0, max_samples_termination=4500000.0, warmup_proportion=0.0, warmup_steps=0.0, start_warmup_step=0.0, local_rank=0, seed=30726, gradient_accumulation_steps=1, fp16=True, loss_scale=0.0, log_freq=0.0, checkpoint_activations=False, resume_from_checkpoint=False, keep_n_most_recent_c
 0: heckpoints=20, num_samples_per_checkpoint=500000, min_samples_to_start_checkpoints=3000000, skip_checkpoint=True, phase2=True, allreduce_post_accumulation=False, allreduce_post_accumulation_fp16=False, do_train=True, exchange_padding=False, unpad=False, unpad_fmha=False, pad_fmha=True, pad=False, enable_fuse_dropout=False, disable_fuse_mask=False, disable_fuse_scale=False, disable_fuse_qkv=False, disable_apex_softmax=False, enable_stream=False, fused_gemm_gelu=True, fused_mha=False, fused_gelu_bias=False, fused_dropout_add=True, fused_bias_mha=True, fused_bias_fc=True, fused_bias_fc_loss_head=False, dense_seq_output=True, use_env=False, bert_config_path='/workspace/phase1/bert_config.json', target_mlm_accuracy=0.72, train_mlm_accuracy_window_size=0, num_epochs_to_generate_seeds_for=2, use_cuda_graph=True, eval_cuda_graph=True, use_ddp=False, ddp_type='apex', use_gradient_as_bucket_view=False, bypass_amp=False, distributed_lamb=True, dwu_group_size=0, dwu_num_blocks=1, dwu_num_chunks=1, dwu_num_rs_pg=1, dwu_nu
 0: m_ar_pg=1, dwu_num_ag_pg=1, dwu_overlap_reductions=False, dwu_e5m2_allgather=False, use_transformer_engine=False, use_transformer_engine2=True, n_gpu=16, device=device(type='cuda', index=0))
 0: Torch distributed is available.
 0: Torch distributed is initialized.
 0: /usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py:153: DeprecationWarning: `interval` argument is deprecated and unused. It will be removed in an upcoming release.
 0:   warnings.warn(
 6: /usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py:153: DeprecationWarning: `interval` argument is deprecated and unused. It will be removed in an upcoming release.
 6:   warnings.warn(
 4: /usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py:153: DeprecationWarning: `interval` argument is deprecated and unused. It will be removed in an upcoming release.
 4:   warnings.warn(
 3: /usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py:153: DeprecationWarning: `interval` argument is deprecated and unused. It will be removed in an upcoming release.
 3:   warnings.warn(
 1: /usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py:153: DeprecationWarning: `interval` argument is deprecated and unused. It will be removed in an upcoming release.
 1:   warnings.warn(
 2: /usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py:153: DeprecationWarning: `interval` argument is deprecated and unused. It will be removed in an upcoming release.
 2:   warnings.warn(
 7: /usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py:153: DeprecationWarning: `interval` argument is deprecated and unused. It will be removed in an upcoming release.
 7:   warnings.warn(
 9: /usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py:153: DeprecationWarning: `interval` argument is deprecated and unused. It will be removed in an upcoming release.
 9:   warnings.warn(
12: /usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py:153: DeprecationWarning: `interval` argument is deprecated and unused. It will be removed in an upcoming release.
12:   warnings.warn(
15: /usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py:153: DeprecationWarning: `interval` argument is deprecated and unused. It will be removed in an upcoming release.
15:   warnings.warn(
 5: /usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py:153: DeprecationWarning: `interval` argument is deprecated and unused. It will be removed in an upcoming release.
 5:   warnings.warn(
 8: /usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py:153: DeprecationWarning: `interval` argument is deprecated and unused. It will be removed in an upcoming release.
 8:   warnings.warn(
13: /usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py:153: DeprecationWarning: `interval` argument is deprecated and unused. It will be removed in an upcoming release.
13:   warnings.warn(
14: /usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py:153: DeprecationWarning: `interval` argument is deprecated and unused. It will be removed in an upcoming release.
14:   warnings.warn(
10: /usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py:153: DeprecationWarning: `interval` argument is deprecated and unused. It will be removed in an upcoming release.
10:   warnings.warn(
11: /usr/local/lib/python3.12/dist-packages/transformer_engine/common/recipe/__init__.py:153: DeprecationWarning: `interval` argument is deprecated and unused. It will be removed in an upcoming release.
11:   warnings.warn(
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438203, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/embeddings/word_embeddings"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438203, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/embeddings/position_embeddings"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438203, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/embeddings/token_type_embeddings"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438203, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/embeddings/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438204, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/embeddings/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438204, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_0/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438204, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_0/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438204, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_0/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438204, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_0/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438204, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_0/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438204, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_0/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438204, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_0/attention/output/dense/kernel"}}
 1: BertForPreTrainingSegmented(
 1:   (bert_model_segment): BertForPreTrainingModelOnly(
 1:     (bert): BertModel(
 1:       (embeddings): BertEmbeddings(
 1:         (word_embeddings): Embedding(30528, 1024)
 1:         (position_embeddings): Embedding(512, 1024)
 1:         (token_type_embeddings): Embedding(2, 1024)
 1:         (LayerNorm): FastLayerNorm()
 1:         (dropout): Dropout(p=0.1, inplace=False)
 1:       )
 1:       (encoder): BertEncoder(
 1:         (layer): ModuleList(
 1:           (0-23): 24 x BertTransformerLayer2(
 1:             (attention): FP8_MHA()
 1:             (layernorm_mlp): LayerNormMLP()
 1:             (output_LayerNorm): FastLayerNorm()
 1:           )
 1:         )
 1:       )
 1:       (pooler): BertPooler(
 1:         (dense): Linear(in_features=1024, out_features=1024, bias=True)
 1:         (activation): Tanh()
 1:       )
 1:     )
 1:   )
 1:   (heads_only_segment): BertForPreTrainingHeadsOnly(
 1:     (cls): BertPreTrainingHeads(
 1:       (predictions): BertLMPredictionHead(
 1:         (transform): BertPredictionHeadTransform(
 1:           (dense): Linear(in_features=1024, out_features=1024, bias=True)
 1:           (LayerNorm): FastLayerNorm()
 1:         )
 1:         (decoder): Linear(in_features=1024, out_features=30528, bias=False)
 1:       )
 1:       (seq_relationship): Linear(in_features=1024, out_features=2, bias=True)
 1:     )
 1:   )
 1: )
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438204, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_0/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438204, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_0/attention/output/LayerNorm/gamma"}}
 6: BertForPreTrainingSegmented(
 6:   (bert_model_segment): BertForPreTrainingModelOnly(
 6:     (bert): BertModel(
 6:       (embeddings): BertEmbeddings(
 6:         (word_embeddings): Embedding(30528, 1024)
 6:         (position_embeddings): Embedding(512, 1024)
 6:         (token_type_embeddings): Embedding(2, 1024)
 6:         (LayerNorm): FastLayerNorm()
 6:         (dropout): Dropout(p=0.1, inplace=False)
 6:       )
 6:       (encoder): BertEncoder(
 6:         (layer): ModuleList(
 6:           (0-23): 24 x BertTransformerLayer2(
 6:             (attention): FP8_MHA()
 6:             (layernorm_mlp): LayerNormMLP()
 6:             (output_LayerNorm): FastLayerNorm()
 6:           )
 6:         )
 6:       )
 6:       (pooler): BertPooler(
 6:         (dense): Linear(in_features=1024, out_features=1024, bias=True)
 6:         (activation): Tanh()
 6:       )
 6:     )
 6:   )
 6:   (heads_only_segment): BertForPreTrainingHeadsOnly(
 6:     (cls): BertPreTrainingHeads(
 6:       (predictions): BertLMPredictionHead(
 6:         (transform): BertPredictionHeadTransform(
 6:           (dense): Linear(in_features=1024, out_features=1024, bias=True)
 6:           (LayerNorm): FastLayerNorm()
 6:         )
 6:         (decoder): Linear(in_features=1024, out_features=30528, bias=False)
 6:       )
 6:       (seq_relationship): Linear(in_features=1024, out_features=2, bias=True)
 6:     )
 6:   )
 6: )
 3: BertForPreTrainingSegmented(
 3:   (bert_model_segment): BertForPreTrainingModelOnly(
 3:     (bert): BertModel(
 3:       (embeddings): BertEmbeddings(
 3:         (word_embeddings): Embedding(30528, 1024)
 3:         (position_embeddings): Embedding(512, 1024)
 3:         (token_type_embeddings): Embedding(2, 1024)
 3:         (LayerNorm): FastLayerNorm()
 3:         (dropout): Dropout(p=0.1, inplace=False)
 3:       )
 3:       (encoder): BertEncoder(
 3:         (layer): ModuleList(
 3:           (0-23): 24 x BertTransformerLayer2(
 3:             (attention): FP8_MHA()
 3:             (layernorm_mlp): LayerNormMLP()
 3:             (output_LayerNorm): FastLayerNorm()
 3:           )
 3:         )
 3:       )
 3:       (pooler): BertPooler(
 3:         (dense): Linear(in_features=1024, out_features=1024, bias=True)
 3:         (activation): Tanh()
 3:       )
 3:     )
 3:   )
 3:   (heads_only_segment): BertForPreTrainingHeadsOnly(
 3:     (cls): BertPreTrainingHeads(
 3:       (predictions): BertLMPredictionHead(
 3:         (transform): BertPredictionHeadTransform(
 3:           (dense): Linear(in_features=1024, out_features=1024, bias=True)
 3:           (LayerNorm): FastLayerNorm()
 3:         )
 3:         (decoder): Linear(in_features=1024, out_features=30528, bias=False)
 3:       )
 3:       (seq_relationship): Linear(in_features=1024, out_features=2, bias=True)
 3:     )
 3:   )
 3: )
 2: BertForPreTrainingSegmented(
 2:   (bert_model_segment): BertForPreTrainingModelOnly(
 2:     (bert): BertModel(
 2:       (embeddings): BertEmbeddings(
 2:         (word_embeddings): Embedding(30528, 1024)
 2:         (position_embeddings): Embedding(512, 1024)
 2:         (token_type_embeddings): Embedding(2, 1024)
 2:         (LayerNorm): FastLayerNorm()
 2:         (dropout): Dropout(p=0.1, inplace=False)
 2:       )
 2:       (encoder): BertEncoder(
 2:         (layer): ModuleList(
 2:           (0-23): 24 x BertTransformerLayer2(
 2:             (attention): FP8_MHA()
 2:             (layernorm_mlp): LayerNormMLP()
 2:             (output_LayerNorm): FastLayerNorm()
 2:           )
 2:         )
 2:       )
 2:       (pooler): BertPooler(
 2:         (dense): Linear(in_features=1024, out_features=1024, bias=True)
 2:         (activation): Tanh()
 2:       )
 2:     )
 2:   )
 2:   (heads_only_segment): BertForPreTrainingHeadsOnly(
 2:     (cls): BertPreTrainingHeads(
 2:       (predictions): BertLMPredictionHead(
 2:         (transform): BertPredictionHeadTransform(
 2:           (dense): Linear(in_features=1024, out_features=1024, bias=True)
 2:           (LayerNorm): FastLayerNorm()
 2:         )
 2:         (decoder): Linear(in_features=1024, out_features=30528, bias=False)
 2:       )
 2:       (seq_relationship): Linear(in_features=1024, out_features=2, bias=True)
 2:     )
 2:   )
 2: )
 7: BertForPreTrainingSegmented(
 7:   (bert_model_segment): BertForPreTrainingModelOnly(
 7:     (bert): BertModel(
 7:       (embeddings): BertEmbeddings(
 7:         (word_embeddings): Embedding(30528, 1024)
 7:         (position_embeddings): Embedding(512, 1024)
 7:         (token_type_embeddings): Embedding(2, 1024)
 7:         (LayerNorm): FastLayerNorm()
 7:         (dropout): Dropout(p=0.1, inplace=False)
 7:       )
 7:       (encoder): BertEncoder(
 7:         (layer): ModuleList(
 7:           (0-23): 24 x BertTransformerLayer2(
 7:             (attention): FP8_MHA()
 7:             (layernorm_mlp): LayerNormMLP()
 7:             (output_LayerNorm): FastLayerNorm()
 7:           )
 7:         )
 7:       )
 7:       (pooler): BertPooler(
 7:         (dense): Linear(in_features=1024, out_features=1024, bias=True)
 7:         (activation): Tanh()
 7:       )
 7:     )
 7:   )
 7:   (heads_only_segment): BertForPreTrainingHeadsOnly(
 7:     (cls): BertPreTrainingHeads(
 7:       (predictions): BertLMPredictionHead(
 7:         (transform): BertPredictionHeadTransform(
 7:           (dense): Linear(in_features=1024, out_features=1024, bias=True)
 7:           (LayerNorm): FastLayerNorm()
 7:         )
 7:         (decoder): Linear(in_features=1024, out_features=30528, bias=False)
 7:       )
 7:       (seq_relationship): Linear(in_features=1024, out_features=2, bias=True)
 7:     )
 7:   )
 7: )
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438205, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_0/attention/output/LayerNorm/beta"}}
 5: BertForPreTrainingSegmented(
 5:   (bert_model_segment): BertForPreTrainingModelOnly(
 5:     (bert): BertModel(
 5:       (embeddings): BertEmbeddings(
 5:         (word_embeddings): Embedding(30528, 1024)
 5:         (position_embeddings): Embedding(512, 1024)
 5:         (token_type_embeddings): Embedding(2, 1024)
 5:         (LayerNorm): FastLayerNorm()
 5:         (dropout): Dropout(p=0.1, inplace=False)
 5:       )
 5:       (encoder): BertEncoder(
 5:         (layer): ModuleList(
 5:           (0-23): 24 x BertTransformerLayer2(
 5:             (attention): FP8_MHA()
 5:             (layernorm_mlp): LayerNormMLP()
 5:             (output_LayerNorm): FastLayerNorm()
 5:           )
 5:         )
 5:       )
 5:       (pooler): BertPooler(
 5:         (dense): Linear(in_features=1024, out_features=1024, bias=True)
 5:         (activation): Tanh()
 5:       )
 5:     )
 5:   )
 5:   (heads_only_segment): BertForPreTrainingHeadsOnly(
 5:     (cls): BertPreTrainingHeads(
 5:       (predictions): BertLMPredictionHead(
 5:         (transform): BertPredictionHeadTransform(
 5:           (dense): Linear(in_features=1024, out_features=1024, bias=True)
 5:           (LayerNorm): FastLayerNorm()
 5:         )
 5:         (decoder): Linear(in_features=1024, out_features=30528, bias=False)
 5:       )
 5:       (seq_relationship): Linear(in_features=1024, out_features=2, bias=True)
 5:     )
 5:   )
 5: )
 4: BertForPreTrainingSegmented(
 4:   (bert_model_segment): BertForPreTrainingModelOnly(
 4:     (bert): BertModel(
 4:       (embeddings): BertEmbeddings(
 4:         (word_embeddings): Embedding(30528, 1024)
 4:         (position_embeddings): Embedding(512, 1024)
 4:         (token_type_embeddings): Embedding(2, 1024)
 4:         (LayerNorm): FastLayerNorm()
 4:         (dropout): Dropout(p=0.1, inplace=False)
 4:       )
 4:       (encoder): BertEncoder(
 4:         (layer): ModuleList(
 4:           (0-23): 24 x BertTransformerLayer2(
 4:             (attention): FP8_MHA()
 4:             (layernorm_mlp): LayerNormMLP()
 4:             (output_LayerNorm): FastLayerNorm()
 4:           )
 4:         )
 4:       )
 4:       (pooler): BertPooler(
 4:         (dense): Linear(in_features=1024, out_features=1024, bias=True)
 4:         (activation): Tanh()
 4:       )
 4:     )
 4:   )
 4:   (heads_only_segment): BertForPreTrainingHeadsOnly(
 4:     (cls): BertPreTrainingHeads(
 4:       (predictions): BertLMPredictionHead(
 4:         (transform): BertPredictionHeadTransform(
 4:           (dense): Linear(in_features=1024, out_features=1024, bias=True)
 4:           (LayerNorm): FastLayerNorm()
 4:         )
 4:         (decoder): Linear(in_features=1024, out_features=30528, bias=False)
 4:       )
 4:       (seq_relationship): Linear(in_features=1024, out_features=2, bias=True)
 4:     )
 4:   )
 4: )
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438205, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_0/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438205, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_0/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438205, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_0/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438205, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_0/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438205, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_0/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438205, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_0/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438205, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_1/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438205, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_1/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438205, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_1/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438206, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_1/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438206, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_1/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438206, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_1/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438206, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_1/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438206, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_1/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438206, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_1/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438206, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_1/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438206, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_1/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438206, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_1/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438206, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_1/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438207, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_1/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438207, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_1/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438207, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_1/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438207, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_2/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438207, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_2/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438207, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_2/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438207, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_2/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438207, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_2/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438207, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_2/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438207, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_2/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438207, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_2/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438208, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_2/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438208, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_2/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438208, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_2/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438208, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_2/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438208, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_2/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438208, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_2/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438208, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_2/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438208, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_2/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438208, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_3/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438208, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_3/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438209, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_3/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438209, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_3/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438209, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_3/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438209, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_3/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438209, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_3/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438209, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_3/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438209, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_3/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438209, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_3/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438209, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_3/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438209, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_3/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438210, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_3/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438210, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_3/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438210, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_3/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438210, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_3/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438210, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_4/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438210, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_4/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438210, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_4/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438210, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_4/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438210, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_4/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438210, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_4/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438210, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_4/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438211, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_4/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438211, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_4/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438211, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_4/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438211, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_4/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438211, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_4/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438211, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_4/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438211, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_4/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438211, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_4/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438211, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_4/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438211, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_5/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438212, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_5/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438212, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_5/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438212, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_5/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438212, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_5/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438212, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_5/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438212, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_5/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438212, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_5/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438212, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_5/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438212, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_5/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438212, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_5/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438213, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_5/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438213, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_5/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438213, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_5/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438213, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_5/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438213, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_5/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438213, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_6/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438213, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_6/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438213, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_6/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438213, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_6/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438213, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_6/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438213, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_6/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438214, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_6/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438214, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_6/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438214, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_6/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438214, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_6/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438214, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_6/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438214, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_6/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438214, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_6/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438214, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_6/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438214, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_6/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438214, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_6/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438215, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_7/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438215, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_7/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438215, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_7/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438215, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_7/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438215, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_7/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438215, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_7/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438215, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_7/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438215, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_7/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438215, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_7/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438215, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_7/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438216, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_7/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438216, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_7/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438216, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_7/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438216, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_7/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438216, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_7/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438216, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_7/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438216, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_8/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438216, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_8/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438216, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_8/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438216, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_8/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438216, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_8/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438217, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_8/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438217, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_8/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438217, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_8/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438217, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_8/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438217, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_8/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438217, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_8/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438217, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_8/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438217, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_8/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438217, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_8/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438217, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_8/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438218, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_8/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438218, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_9/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438218, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_9/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438218, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_9/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438218, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_9/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438218, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_9/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438218, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_9/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438218, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_9/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438218, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_9/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438218, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_9/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438218, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_9/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438219, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_9/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438219, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_9/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438219, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_9/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438219, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_9/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438219, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_9/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438219, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_9/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438219, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_10/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438219, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_10/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438219, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_10/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438219, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_10/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438220, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_10/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438220, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_10/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438220, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_10/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438220, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_10/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438220, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_10/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438220, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_10/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438220, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_10/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438220, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_10/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438220, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_10/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438220, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_10/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438221, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_10/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438221, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_10/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438221, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_11/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438221, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_11/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438221, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_11/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438221, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_11/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438221, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_11/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438221, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_11/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438221, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_11/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438221, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_11/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438221, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_11/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438222, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_11/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438222, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_11/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438222, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_11/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438222, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_11/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438222, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_11/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438222, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_11/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438222, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_11/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438222, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_12/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438222, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_12/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438222, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_12/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438223, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_12/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438223, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_12/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438223, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_12/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438223, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_12/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438223, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_12/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438223, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_12/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438223, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_12/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438223, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_12/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438223, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_12/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438223, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_12/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438224, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_12/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438224, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_12/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438224, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_12/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438224, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_13/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438224, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_13/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438224, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_13/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438224, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_13/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438224, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_13/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438224, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_13/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438224, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_13/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438224, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_13/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438225, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_13/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438225, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_13/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438225, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_13/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438225, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_13/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438225, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_13/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438225, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_13/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438225, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_13/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438225, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_13/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438225, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_14/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438225, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_14/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438226, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_14/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438226, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_14/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438226, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_14/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438226, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_14/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438226, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_14/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438226, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_14/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438226, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_14/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438226, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_14/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438226, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_14/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438226, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_14/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438226, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_14/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438227, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_14/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438227, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_14/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438227, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_14/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438227, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_15/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438227, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_15/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438227, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_15/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438227, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_15/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438227, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_15/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438227, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_15/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438227, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_15/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438228, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_15/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438228, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_15/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438228, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_15/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438228, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_15/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438228, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_15/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438228, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_15/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438228, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_15/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438228, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_15/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438228, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_15/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438228, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_16/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438229, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_16/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438229, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_16/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438229, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_16/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438229, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_16/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438229, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_16/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438229, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_16/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438229, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_16/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438229, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_16/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438229, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_16/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438229, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_16/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438229, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_16/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438230, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_16/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438230, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_16/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438230, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_16/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438230, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_16/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438230, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_17/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438230, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_17/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438230, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_17/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438230, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_17/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438230, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_17/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438230, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_17/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438231, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_17/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438231, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_17/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438231, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_17/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438231, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_17/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438231, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_17/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438231, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_17/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438231, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_17/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438231, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_17/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438231, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_17/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438231, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_17/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438232, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_18/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438232, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_18/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438232, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_18/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438232, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_18/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438232, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_18/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438232, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_18/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438232, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_18/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438232, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_18/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438232, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_18/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438232, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_18/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438233, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_18/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438233, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_18/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438233, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_18/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438233, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_18/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438233, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_18/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438233, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_18/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438233, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_19/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438233, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_19/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438233, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_19/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438233, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_19/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438233, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_19/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438234, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_19/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438234, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_19/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438234, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_19/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438234, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_19/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438234, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_19/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438234, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_19/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438234, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_19/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438234, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_19/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438234, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_19/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438234, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_19/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438235, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_19/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438235, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_20/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438235, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_20/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438235, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_20/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438235, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_20/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438235, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_20/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438235, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_20/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438235, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_20/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438235, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_20/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438235, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_20/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438235, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_20/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438236, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_20/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438236, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_20/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438236, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_20/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438236, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_20/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438236, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_20/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438236, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_20/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438236, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_21/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438236, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_21/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438236, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_21/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438236, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_21/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438237, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_21/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438237, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_21/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438237, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_21/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438237, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_21/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438237, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_21/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438237, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_21/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438237, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_21/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438237, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_21/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438237, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_21/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438237, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_21/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438238, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_21/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438238, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_21/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438238, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_22/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438238, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_22/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438238, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_22/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438238, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_22/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438238, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_22/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438238, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_22/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438238, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_22/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438238, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_22/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438238, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_22/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438239, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_22/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438239, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_22/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438239, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_22/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438239, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_22/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438239, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_22/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438239, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_22/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438239, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_22/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438239, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_23/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438239, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_23/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438239, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_23/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438240, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_23/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438240, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_23/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438240, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_23/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438240, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_23/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438240, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_23/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438240, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_23/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438240, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_23/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438240, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_23/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438240, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_23/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438240, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_23/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438241, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_23/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438241, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_23/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438241, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/encoder/layer_23/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438241, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/pooler/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438241, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "bert/pooler/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438241, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "cls/predictions/output_bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438241, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "cls/predictions/transform/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438241, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "cls/predictions/transform/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438241, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "cls/predictions/transform/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438241, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "cls/predictions/transform/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438241, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "cls/seq_relationship/output_weights"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438242, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "tensor": "cls/seq_relationship/output_bias"}}
 0: BertForPreTrainingSegmented(
 0:   (bert_model_segment): BertForPreTrainingModelOnly(
 0:     (bert): BertModel(
 0:       (embeddings): BertEmbeddings(
 0:         (word_embeddings): Embedding(30528, 1024)
 0:         (position_embeddings): Embedding(512, 1024)
 0:         (token_type_embeddings): Embedding(2, 1024)
 0:         (LayerNorm): FastLayerNorm()
 0:         (dropout): Dropout(p=0.1, inplace=False)
 0:       )
 0:       (encoder): BertEncoder(
 0:         (layer): ModuleList(
 0:           (0-23): 24 x BertTransformerLayer2(
 0:             (attention): FP8_MHA()
 0:             (layernorm_mlp): LayerNormMLP()
 0:             (output_LayerNorm): FastLayerNorm()
 0:           )
 0:         )
 0:       )
 0:       (pooler): BertPooler(
 0:         (dense): Linear(in_features=1024, out_features=1024, bias=True)
 0:         (activation): Tanh()
 0:       )
 0:     )
 0:   )
 0:   (heads_only_segment): BertForPreTrainingHeadsOnly(
 0:     (cls): BertPreTrainingHeads(
 0:       (predictions): BertLMPredictionHead(
 0:         (transform): BertPredictionHeadTransform(
 0:           (dense): Linear(in_features=1024, out_features=1024, bias=True)
 0:           (LayerNorm): FastLayerNorm()
 0:         )
 0:         (decoder): Linear(in_features=1024, out_features=30528, bias=False)
 0:       )
 0:       (seq_relationship): Linear(in_features=1024, out_features=2, bias=True)
 0:     )
 0:   )
 0: )
11: BertForPreTrainingSegmented(
11:   (bert_model_segment): BertForPreTrainingModelOnly(
11:     (bert): BertModel(
11:       (embeddings): BertEmbeddings(
11:         (word_embeddings): Embedding(30528, 1024)
11:         (position_embeddings): Embedding(512, 1024)
11:         (token_type_embeddings): Embedding(2, 1024)
11:         (LayerNorm): FastLayerNorm()
11:         (dropout): Dropout(p=0.1, inplace=False)
11:       )
11:       (encoder): BertEncoder(
11:         (layer): ModuleList(
11:           (0-23): 24 x BertTransformerLayer2(
11:             (attention): FP8_MHA()
11:             (layernorm_mlp): LayerNormMLP()
11:             (output_LayerNorm): FastLayerNorm()
11:           )
11:         )
11:       )
11:       (pooler): BertPooler(
11:         (dense): Linear(in_features=1024, out_features=1024, bias=True)
11:         (activation): Tanh()
11:       )
11:     )
11:   )
11:   (heads_only_segment): BertForPreTrainingHeadsOnly(
11:     (cls): BertPreTrainingHeads(
11:       (predictions): BertLMPredictionHead(
11:         (transform): BertPredictionHeadTransform(
11:           (dense): Linear(in_features=1024, out_features=1024, bias=True)
11:           (LayerNorm): FastLayerNorm()
11:         )
11:         (decoder): Linear(in_features=1024, out_features=30528, bias=False)
11:       )
11:       (seq_relationship): Linear(in_features=1024, out_features=2, bias=True)
11:     )
11:   )
11: )
10: BertForPreTrainingSegmented(
10:   (bert_model_segment): BertForPreTrainingModelOnly(
10:     (bert): BertModel(
10:       (embeddings): BertEmbeddings(
10:         (word_embeddings): Embedding(30528, 1024)
10:         (position_embeddings): Embedding(512, 1024)
10:         (token_type_embeddings): Embedding(2, 1024)
10:         (LayerNorm): FastLayerNorm()
10:         (dropout): Dropout(p=0.1, inplace=False)
10:       )
10:       (encoder): BertEncoder(
10:         (layer): ModuleList(
10:           (0-23): 24 x BertTransformerLayer2(
10:             (attention): FP8_MHA()
10:             (layernorm_mlp): LayerNormMLP()
10:             (output_LayerNorm): FastLayerNorm()
10:           )
10:         )
10:       )
10:       (pooler): BertPooler(
10:         (dense): Linear(in_features=1024, out_features=1024, bias=True)
10:         (activation): Tanh()
10:       )
10:     )
10:   )
10:   (heads_only_segment): BertForPreTrainingHeadsOnly(
10:     (cls): BertPreTrainingHeads(
10:       (predictions): BertLMPredictionHead(
10:         (transform): BertPredictionHeadTransform(
10:           (dense): Linear(in_features=1024, out_features=1024, bias=True)
10:           (LayerNorm): FastLayerNorm()
10:         )
10:         (decoder): Linear(in_features=1024, out_features=30528, bias=False)
10:       )
10:       (seq_relationship): Linear(in_features=1024, out_features=2, bias=True)
10:     )
10:   )
10: )
13: BertForPreTrainingSegmented(
13:   (bert_model_segment): BertForPreTrainingModelOnly(
13:     (bert): BertModel(
13:       (embeddings): BertEmbeddings(
13:         (word_embeddings): Embedding(30528, 1024)
13:         (position_embeddings): Embedding(512, 1024)
13:         (token_type_embeddings): Embedding(2, 1024)
13:         (LayerNorm): FastLayerNorm()
13:         (dropout): Dropout(p=0.1, inplace=False)
13:       )
13:       (encoder): BertEncoder(
13:         (layer): ModuleList(
13:           (0-23): 24 x BertTransformerLayer2(
13:             (attention): FP8_MHA()
13:             (layernorm_mlp): LayerNormMLP()
13:             (output_LayerNorm): FastLayerNorm()
13:           )
13:         )
13:       )
13:       (pooler): BertPooler(
13:         (dense): Linear(in_features=1024, out_features=1024, bias=True)
13:         (activation): Tanh()
13:       )
13:     )
13:   )
13:   (heads_only_segment): BertForPreTrainingHeadsOnly(
13:     (cls): BertPreTrainingHeads(
13:       (predictions): BertLMPredictionHead(
13:         (transform): BertPredictionHeadTransform(
13:           (dense): Linear(in_features=1024, out_features=1024, bias=True)
13:           (LayerNorm): FastLayerNorm()
13:         )
13:         (decoder): Linear(in_features=1024, out_features=30528, bias=False)
13:       )
13:       (seq_relationship): Linear(in_features=1024, out_features=2, bias=True)
13:     )
13:   )
13: )
 8: BertForPreTrainingSegmented(
 8:   (bert_model_segment): BertForPreTrainingModelOnly(
 8:     (bert): BertModel(
 8:       (embeddings): BertEmbeddings(
 8:         (word_embeddings): Embedding(30528, 1024)
 8:         (position_embeddings): Embedding(512, 1024)
 8:         (token_type_embeddings): Embedding(2, 1024)
 8:         (LayerNorm): FastLayerNorm()
 8:         (dropout): Dropout(p=0.1, inplace=False)
 8:       )
 8:       (encoder): BertEncoder(
 8:         (layer): ModuleList(
 8:           (0-23): 24 x BertTransformerLayer2(
 8:             (attention): FP8_MHA()
 8:             (layernorm_mlp): LayerNormMLP()
 8:             (output_LayerNorm): FastLayerNorm()
 8:           )
 8:         )
 8:       )
 8:       (pooler): BertPooler(
 8:         (dense): Linear(in_features=1024, out_features=1024, bias=True)
 8:         (activation): Tanh()
 8:       )
 8:     )
 8:   )
 8:   (heads_only_segment): BertForPreTrainingHeadsOnly(
 8:     (cls): BertPreTrainingHeads(
 8:       (predictions): BertLMPredictionHead(
 8:         (transform): BertPredictionHeadTransform(
 8:           (dense): Linear(in_features=1024, out_features=1024, bias=True)
 8:           (LayerNorm): FastLayerNorm()
 8:         )
 8:         (decoder): Linear(in_features=1024, out_features=30528, bias=False)
 8:       )
 8:       (seq_relationship): Linear(in_features=1024, out_features=2, bias=True)
 8:     )
 8:   )
 8: )
12: BertForPreTrainingSegmented(
12:   (bert_model_segment): BertForPreTrainingModelOnly(
12:     (bert): BertModel(
12:       (embeddings): BertEmbeddings(
12:         (word_embeddings): Embedding(30528, 1024)
12:         (position_embeddings): Embedding(512, 1024)
12:         (token_type_embeddings): Embedding(2, 1024)
12:         (LayerNorm): FastLayerNorm()
12:         (dropout): Dropout(p=0.1, inplace=False)
12:       )
12:       (encoder): BertEncoder(
12:         (layer): ModuleList(
12:           (0-23): 24 x BertTransformerLayer2(
12:             (attention): FP8_MHA()
12:             (layernorm_mlp): LayerNormMLP()
12:             (output_LayerNorm): FastLayerNorm()
12:           )
12:         )
12:       )
12:       (pooler): BertPooler(
12:         (dense): Linear(in_features=1024, out_features=1024, bias=True)
12:         (activation): Tanh()
12:       )
12:     )
12:   )
12:   (heads_only_segment): BertForPreTrainingHeadsOnly(
12:     (cls): BertPreTrainingHeads(
12:       (predictions): BertLMPredictionHead(
12:         (transform): BertPredictionHeadTransform(
12:           (dense): Linear(in_features=1024, out_features=1024, bias=True)
12:           (LayerNorm): FastLayerNorm()
12:         )
12:         (decoder): Linear(in_features=1024, out_features=30528, bias=False)
12:       )
12:       (seq_relationship): Linear(in_features=1024, out_features=2, bias=True)
12:     )
12:   )
12: )
14: BertForPreTrainingSegmented(
14:   (bert_model_segment): BertForPreTrainingModelOnly(
14:     (bert): BertModel(
14:       (embeddings): BertEmbeddings(
14:         (word_embeddings): Embedding(30528, 1024)
14:         (position_embeddings): Embedding(512, 1024)
14:         (token_type_embeddings): Embedding(2, 1024)
14:         (LayerNorm): FastLayerNorm()
14:         (dropout): Dropout(p=0.1, inplace=False)
14:       )
14:       (encoder): BertEncoder(
14:         (layer): ModuleList(
14:           (0-23): 24 x BertTransformerLayer2(
14:             (attention): FP8_MHA()
14:             (layernorm_mlp): LayerNormMLP()
14:             (output_LayerNorm): FastLayerNorm()
14:           )
14:         )
14:       )
14:       (pooler): BertPooler(
14:         (dense): Linear(in_features=1024, out_features=1024, bias=True)
14:         (activation): Tanh()
14:       )
14:     )
14:   )
14:   (heads_only_segment): BertForPreTrainingHeadsOnly(
14:     (cls): BertPreTrainingHeads(
14:       (predictions): BertLMPredictionHead(
14:         (transform): BertPredictionHeadTransform(
14:           (dense): Linear(in_features=1024, out_features=1024, bias=True)
14:           (LayerNorm): FastLayerNorm()
14:         )
14:         (decoder): Linear(in_features=1024, out_features=30528, bias=False)
14:       )
14:       (seq_relationship): Linear(in_features=1024, out_features=2, bias=True)
14:     )
14:   )
14: )
15: BertForPreTrainingSegmented(
15:   (bert_model_segment): BertForPreTrainingModelOnly(
15:     (bert): BertModel(
15:       (embeddings): BertEmbeddings(
15:         (word_embeddings): Embedding(30528, 1024)
15:         (position_embeddings): Embedding(512, 1024)
15:         (token_type_embeddings): Embedding(2, 1024)
15:         (LayerNorm): FastLayerNorm()
15:         (dropout): Dropout(p=0.1, inplace=False)
15:       )
15:       (encoder): BertEncoder(
15:         (layer): ModuleList(
15:           (0-23): 24 x BertTransformerLayer2(
15:             (attention): FP8_MHA()
15:             (layernorm_mlp): LayerNormMLP()
15:             (output_LayerNorm): FastLayerNorm()
15:           )
15:         )
15:       )
15:       (pooler): BertPooler(
15:         (dense): Linear(in_features=1024, out_features=1024, bias=True)
15:         (activation): Tanh()
15:       )
15:     )
15:   )
15:   (heads_only_segment): BertForPreTrainingHeadsOnly(
15:     (cls): BertPreTrainingHeads(
15:       (predictions): BertLMPredictionHead(
15:         (transform): BertPredictionHeadTransform(
15:           (dense): Linear(in_features=1024, out_features=1024, bias=True)
15:           (LayerNorm): FastLayerNorm()
15:         )
15:         (decoder): Linear(in_features=1024, out_features=30528, bias=False)
15:       )
15:       (seq_relationship): Linear(in_features=1024, out_features=2, bias=True)
15:     )
15:   )
15: )
 9: BertForPreTrainingSegmented(
 9:   (bert_model_segment): BertForPreTrainingModelOnly(
 9:     (bert): BertModel(
 9:       (embeddings): BertEmbeddings(
 9:         (word_embeddings): Embedding(30528, 1024)
 9:         (position_embeddings): Embedding(512, 1024)
 9:         (token_type_embeddings): Embedding(2, 1024)
 9:         (LayerNorm): FastLayerNorm()
 9:         (dropout): Dropout(p=0.1, inplace=False)
 9:       )
 9:       (encoder): BertEncoder(
 9:         (layer): ModuleList(
 9:           (0-23): 24 x BertTransformerLayer2(
 9:             (attention): FP8_MHA()
 9:             (layernorm_mlp): LayerNormMLP()
 9:             (output_LayerNorm): FastLayerNorm()
 9:           )
 9:         )
 9:       )
 9:       (pooler): BertPooler(
 9:         (dense): Linear(in_features=1024, out_features=1024, bias=True)
 9:         (activation): Tanh()
 9:       )
 9:     )
 9:   )
 9:   (heads_only_segment): BertForPreTrainingHeadsOnly(
 9:     (cls): BertPreTrainingHeads(
 9:       (predictions): BertLMPredictionHead(
 9:         (transform): BertPredictionHeadTransform(
 9:           (dense): Linear(in_features=1024, out_features=1024, bias=True)
 9:           (LayerNorm): FastLayerNorm()
 9:         )
 9:         (decoder): Linear(in_features=1024, out_features=30528, bias=False)
 9:       )
 9:       (seq_relationship): Linear(in_features=1024, out_features=2, bias=True)
 9:     )
 9:   )
 9: )
 7: /usr/local/lib/python3.12/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 7:   self._overflow_buf = torch.cuda.IntTensor([0])
 5: /usr/local/lib/python3.12/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 5:   self._overflow_buf = torch.cuda.IntTensor([0])
 1: /usr/local/lib/python3.12/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 1:   self._overflow_buf = torch.cuda.IntTensor([0])
 6: /usr/local/lib/python3.12/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 6:   self._overflow_buf = torch.cuda.IntTensor([0])
 4: /usr/local/lib/python3.12/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 4:   self._overflow_buf = torch.cuda.IntTensor([0])
15: /usr/local/lib/python3.12/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
15:   self._overflow_buf = torch.cuda.IntTensor([0])
 3: /usr/local/lib/python3.12/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 3:   self._overflow_buf = torch.cuda.IntTensor([0])
 2: /usr/local/lib/python3.12/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 2:   self._overflow_buf = torch.cuda.IntTensor([0])
12: /usr/local/lib/python3.12/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
12:   self._overflow_buf = torch.cuda.IntTensor([0])
 9: /usr/local/lib/python3.12/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 9:   self._overflow_buf = torch.cuda.IntTensor([0])
 0: :::MLLOG {"namespace": "", "time_ms": 1746134438497, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.00096, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 991}}
 0: /usr/local/lib/python3.12/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 0:   self._overflow_buf = torch.cuda.IntTensor([0])
13: /usr/local/lib/python3.12/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
13:   self._overflow_buf = torch.cuda.IntTensor([0])
 8: /usr/local/lib/python3.12/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
 8:   self._overflow_buf = torch.cuda.IntTensor([0])
11: /usr/local/lib/python3.12/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
11:   self._overflow_buf = torch.cuda.IntTensor([0])
10: /usr/local/lib/python3.12/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
10:   self._overflow_buf = torch.cuda.IntTensor([0])
14: /usr/local/lib/python3.12/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
14:   self._overflow_buf = torch.cuda.IntTensor([0])
 0: [rank0]:[W501 21:20:40.351169464 ProcessGroupNCCL.cpp:4751] [PG ID 3 PG GUID 4 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
 1: [rank1]:[W501 21:20:40.351161488 ProcessGroupNCCL.cpp:4751] [PG ID 3 PG GUID 4 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
 3: [rank3]:[W501 21:20:40.351166489 ProcessGroupNCCL.cpp:4751] [PG ID 3 PG GUID 4 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
 4: [rank4]:[W501 21:20:40.351289420 ProcessGroupNCCL.cpp:4751] [PG ID 3 PG GUID 4 Rank 4]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
 8: [rank8]:[W501 21:20:43.200127376 ProcessGroupNCCL.cpp:4751] [PG ID 3 PG GUID 5 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
 2: [rank2]:[W501 21:20:40.351349363 ProcessGroupNCCL.cpp:4751] [PG ID 3 PG GUID 4 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
 6: [rank6]:[W501 21:20:40.351362111 ProcessGroupNCCL.cpp:4751] [PG ID 3 PG GUID 4 Rank 6]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
 9: [rank9]:[W501 21:20:43.200136867 ProcessGroupNCCL.cpp:4751] [PG ID 3 PG GUID 5 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
11: [rank11]:[W501 21:20:43.200321718 ProcessGroupNCCL.cpp:4751] [PG ID 3 PG GUID 5 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
 5: [rank5]:[W501 21:20:40.351415017 ProcessGroupNCCL.cpp:4751] [PG ID 3 PG GUID 4 Rank 5]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
14: [rank14]:[W501 21:20:43.200347068 ProcessGroupNCCL.cpp:4751] [PG ID 3 PG GUID 5 Rank 6]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
13: [rank13]:[W501 21:20:43.200347748 ProcessGroupNCCL.cpp:4751] [PG ID 3 PG GUID 5 Rank 5]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
10: [rank10]:[W501 21:20:43.200456106 ProcessGroupNCCL.cpp:4751] [PG ID 3 PG GUID 5 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
 7: [rank7]:[W501 21:20:40.351710614 ProcessGroupNCCL.cpp:4751] [PG ID 3 PG GUID 4 Rank 7]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
12: [rank12]:[W501 21:20:43.200667242 ProcessGroupNCCL.cpp:4751] [PG ID 3 PG GUID 5 Rank 4]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
15: [rank15]:[W501 21:20:43.200663421 ProcessGroupNCCL.cpp:4751] [PG ID 3 PG GUID 5 Rank 7]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
 0: :::MLLOG {"namespace": "", "time_ms": 1746134442517, "event_type": "POINT_IN_TIME", "key": "opt_lamb_epsilon", "value": 1e-06, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1030}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134442518, "event_type": "POINT_IN_TIME", "key": "opt_epsilon", "value": 1e-06, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1031}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134442518, "event_type": "POINT_IN_TIME", "key": "opt_lamb_beta_1", "value": 0.60466, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1033}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134442518, "event_type": "POINT_IN_TIME", "key": "opt_lamb_beta_2", "value": 0.85437, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1034}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134442518, "event_type": "POINT_IN_TIME", "key": "opt_lamb_weight_decay_rate", "value": 0.1, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1035}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134442574, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 86}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134442575, "event_type": "POINT_IN_TIME", "key": "opt_lamb_learning_rate_decay_poly_power", "value": 1.0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 87}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134442575, "event_type": "POINT_IN_TIME", "key": "start_warmup_step", "value": 0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 88}}
 9: Torch distributed is available.
 9: Torch distributed is initialized.
12: Torch distributed is available.
12: Torch distributed is initialized.
10: Torch distributed is available.
10: Torch distributed is initialized.
13: Torch distributed is available.
13: Torch distributed is initialized.
14: Torch distributed is available.
14: Torch distributed is initialized.
15: Torch distributed is available.
15: Torch distributed is initialized.
 8: Torch distributed is available.
 8: Torch distributed is initialized.
11: Torch distributed is available.
11: Torch distributed is initialized.
 7: Torch distributed is available.
 7: Torch distributed is initialized.
 0: Torch distributed is available.
 0: Torch distributed is initialized.
 1: Torch distributed is available.
 1: Torch distributed is initialized.
 5: Torch distributed is available.
 5: Torch distributed is initialized.
 6: Torch distributed is available.
 6: Torch distributed is initialized.
 3: Torch distributed is available.
 3: Torch distributed is initialized.
 4: Torch distributed is available.
 4: Torch distributed is initialized.
 2: Torch distributed is available.
 2: Torch distributed is initialized.
14: Enabling make_graphed_callables for encoder!!
13: Enabling make_graphed_callables for encoder!!
 9: Enabling make_graphed_callables for encoder!!
10: Enabling make_graphed_callables for encoder!!
15: Enabling make_graphed_callables for encoder!!
12: Enabling make_graphed_callables for encoder!!
 6: Enabling make_graphed_callables for encoder!!
 4: Enabling make_graphed_callables for encoder!!
11: Enabling make_graphed_callables for encoder!!
 8: Enabling make_graphed_callables for encoder!!
 7: Enabling make_graphed_callables for encoder!!
 5: Enabling make_graphed_callables for encoder!!
 1: Enabling make_graphed_callables for encoder!!
 0: Enabling make_graphed_callables for encoder!!
 3: Enabling make_graphed_callables for encoder!!
 2: Enabling make_graphed_callables for encoder!!
 3: /usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:824: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
 3:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 1: /usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:824: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
 1:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 6: /usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:824: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
 6:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 0: /usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:824: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
 0:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 2: /usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:824: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
 2:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 5: /usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:824: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
 5:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 7: /usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:824: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
 7:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 4: /usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:824: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
 4:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 8: /usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:824: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
 8:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
10: /usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:824: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
10:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
11: /usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:824: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
11:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
13: /usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:824: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
13:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
14: /usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:824: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
14:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
12: /usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:824: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
12:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
15: /usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:824: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
15:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
 9: /usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:824: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
 9:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
11: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
11:   warnings.warn(
 8: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 8:   warnings.warn(
15: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
15:   warnings.warn(
 9: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 9:   warnings.warn(
 0: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 0:   warnings.warn(
 1: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 1:   warnings.warn(
 2: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 2:   warnings.warn(
 4: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 4:   warnings.warn(
12: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
12:   warnings.warn(
 6: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 6:   warnings.warn(
13: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
13:   warnings.warn(
14: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
14:   warnings.warn(
 5: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 5:   warnings.warn(
 7: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 7:   warnings.warn(
10: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
10:   warnings.warn(
 3: /usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:419: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 3:   warnings.warn(
 0: :::MLLOG {"namespace": "", "time_ms": 1746134446877, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1783}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134446878, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1783}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134446897, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1805, "epoch_num": 0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134446898, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1807, "first_epoch_num": 1, "epoch_count": 1}}
 0: parsed args:
 0: Namespace(input_dir='/workspace/data_phase2', packed_samples=True, order_samples=False, max_pack_factor=3, average_packing_rate=2, synthetic_input=False, bert_model='bert-large-uncased', cuda_graph_mode='segmented', max_iterations_per_graph=4, output_dir='/results', eval_dir='/workspace/evaldata', eval_iter_start_samples=150000, eval_iter_samples=150000, num_eval_examples=10000, cache_eval_data=True, load_eval_synchronously=False, init_checkpoint='/workspace/phase1/model.ckpt-28252.pt', init_tf_checkpoint=None, max_seq_length=512, max_predictions_per_seq=76, train_batch_size=48, eval_batch_size=16, learning_rate=0.00096, weight_decay_rate=0.1, opt_lamb_beta_1=0.60466, opt_lamb_beta_2=0.85437, max_steps=3680.0, sustained_training_time=0, max_samples_termination=4500000.0, warmup_proportion=0.0, warmup_steps=0.0, start_warmup_step=0.0, local_rank=0, seed=30726, gradient_accumulation_steps=1, fp16=True, loss_scale=0.0, log_freq=0.0, checkpoint_activations=False, resume_from_checkpoint=False, keep_n_most_recent_c
 0: heckpoints=20, num_samples_per_checkpoint=500000, min_samples_to_start_checkpoints=3000000, skip_checkpoint=True, phase2=True, allreduce_post_accumulation=False, allreduce_post_accumulation_fp16=False, do_train=True, exchange_padding=False, unpad=False, unpad_fmha=False, pad_fmha=True, pad=False, enable_fuse_dropout=False, disable_fuse_mask=False, disable_fuse_scale=False, disable_fuse_qkv=False, disable_apex_softmax=False, enable_stream=False, fused_gemm_gelu=True, fused_mha=False, fused_gelu_bias=False, fused_dropout_add=True, fused_bias_mha=True, fused_bias_fc=True, fused_bias_fc_loss_head=False, dense_seq_output=True, use_env=False, bert_config_path='/workspace/phase1/bert_config.json', target_mlm_accuracy=0.72, train_mlm_accuracy_window_size=0, num_epochs_to_generate_seeds_for=2, use_cuda_graph=True, eval_cuda_graph=True, use_ddp=False, ddp_type='apex', use_gradient_as_bucket_view=False, bypass_amp=False, distributed_lamb=True, dwu_group_size=0, dwu_num_blocks=1, dwu_num_chunks=1, dwu_num_rs_pg=1, dwu_nu
 0: m_ar_pg=1, dwu_num_ag_pg=1, dwu_overlap_reductions=False, dwu_e5m2_allgather=False, use_transformer_engine=False, use_transformer_engine2=True, n_gpu=16, device=device(type='cuda', index=0), resume_step=0)
 0: epoch: 1
 0: :::MLLOG {"namespace": "", "time_ms": 1746134446899, "event_type": "POINT_IN_TIME", "key": "data_file", "value": "/workspace/data_phase2/part_00715", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1844}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134454031, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 21027.26871461905}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 149959}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134454032, "event_type": "INTERVAL_START", "key": "eval_start", "value": 149959, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 149959}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134454598, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 149959, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 149959}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134454599, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.3740002512931824, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 149959}}
 0: {'global_steps': 98, 'eval_loss': 4.105368137359619, 'eval_mlm_accuracy': 0.3740002512931824}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134460836, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 22077.65384997253}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 300206}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134460837, "event_type": "INTERVAL_START", "key": "eval_start", "value": 300206, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 300206}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134461145, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 300206, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 300206}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134461146, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.38361647725105286, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 300206}}
 0: {'global_steps': 196, 'eval_loss': 3.996166467666626, 'eval_mlm_accuracy': 0.38361647725105286}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134467291, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 23052.689458813205}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 449015}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134467292, "event_type": "INTERVAL_START", "key": "eval_start", "value": 449015, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 449015}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134467596, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 449015, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 449015}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134467597, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.4018190801143646, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 449015}}
 0: {'global_steps': 293, 'eval_loss': 3.839134454727173, 'eval_mlm_accuracy': 0.4018190801143646}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134474314, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 21388.28557840728}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 599227}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134474316, "event_type": "INTERVAL_START", "key": "eval_start", "value": 599227, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 599227}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134474617, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 599227, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 599227}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134474618, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.43769192695617676, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 599227}}
 0: {'global_steps': 391, 'eval_loss': 3.4904019832611084, 'eval_mlm_accuracy': 0.43769192695617676}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134480847, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 23071.81930532674}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 749938}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134480848, "event_type": "INTERVAL_START", "key": "eval_start", "value": 749938, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 749938}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134481151, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 749938, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 749938}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134481152, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.5128142237663269, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 749938}}
 0: {'global_steps': 489, 'eval_loss': 2.8657431602478027, 'eval_mlm_accuracy': 0.5128142237663269}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134487307, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 23064.928329699625}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 898946}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134487308, "event_type": "INTERVAL_START", "key": "eval_start", "value": 898946, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 898946}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134487612, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 898946, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 898946}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134487613, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.6110336780548096, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 898946}}
 0: {'global_steps': 586, 'eval_loss': 2.0665671825408936, 'eval_mlm_accuracy': 0.6110336780548096}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134493847, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 22916.89311262061}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 1048816}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134493848, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1048816, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 1048816}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134494152, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1048816, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 1048816}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134494153, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.6860718727111816, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 1048816}}
 0: {'global_steps': 684, 'eval_loss': 1.5253441333770752, 'eval_mlm_accuracy': 0.6860718727111816}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134500892, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 21358.107508623667}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 1199290}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134500894, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1199290, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 1199290}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134501201, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1199290, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 1199290}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134501202, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7034058570861816, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 1199290}}
 0: {'global_steps': 782, 'eval_loss': 1.404800534248352, 'eval_mlm_accuracy': 0.7034058570861816}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134507342, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 23132.885850060575}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 1348506}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134507344, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1348506, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 1348506}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134507647, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1348506, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 1348506}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134507648, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7084988355636597, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 1348506}}
 0: {'global_steps': 879, 'eval_loss': 1.3757201433181763, 'eval_mlm_accuracy': 0.7084988355636597}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134513873, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 23002.56042588218}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 1498724}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134513874, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1498724, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 1498724}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134514178, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1498724, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 1498724}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134514179, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7115556001663208, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 1498724}}
 0: {'global_steps': 977, 'eval_loss': 1.358796238899231, 'eval_mlm_accuracy': 0.7115556001663208}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134520393, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 22986.651521183532}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 1648595}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134520394, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1648595, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 1648595}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134520696, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1648595, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 1648595}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134520697, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7132439017295837, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 1648595}}
 0: {'global_steps': 1075, 'eval_loss': 1.3458027839660645, 'eval_mlm_accuracy': 0.7132439017295837}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134527393, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 21242.17564924818}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 1797304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134527395, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1797304, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 1797304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134527701, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1797304, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 1797304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134527702, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7142130136489868, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 1797304}}
 0: {'global_steps': 1172, 'eval_loss': 1.3392945528030396, 'eval_mlm_accuracy': 0.7142130136489868}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134533924, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 22984.310519827846}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 1947403}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134533925, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1947403, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 1947403}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134534227, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1947403, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 1947403}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134534228, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.715672492980957, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 1947403}}
 0: {'global_steps': 1270, 'eval_loss': 1.3321146965026855, 'eval_mlm_accuracy': 0.715672492980957}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134540450, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 23027.77635537423}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 2097682}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134540451, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2097682, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 2097682}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134540754, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2097682, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 2097682}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134540755, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7164453864097595, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 2097682}}
 0: {'global_steps': 1368, 'eval_loss': 1.3277099132537842, 'eval_mlm_accuracy': 0.7164453864097595}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134546919, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 23036.042656170164}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 2246696}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134546920, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2246696, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 2246696}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134547223, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2246696, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 2246696}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134547224, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7170829176902771, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 2246696}}
 0: {'global_steps': 1465, 'eval_loss': 1.3248400688171387, 'eval_mlm_accuracy': 0.7170829176902771}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134553924, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 21484.69498984178}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 2397204}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134553925, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2397204, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 2397204}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134554231, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2397204, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 2397204}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134554232, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7179842591285706, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 2397204}}
 0: {'global_steps': 1563, 'eval_loss': 1.318163514137268, 'eval_mlm_accuracy': 0.7179842591285706}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134560468, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 22899.863044815313}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 2547057}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134560469, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2547057, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 2547057}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134560775, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2547057, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 2547057}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134560776, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7184279561042786, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 2547057}}
 0: {'global_steps': 1661, 'eval_loss': 1.3171135187149048, 'eval_mlm_accuracy': 0.7184279561042786}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134566935, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 23006.865324158098}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 2695857}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134566937, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2695857, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 2695857}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134567240, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2695857, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 2695857}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134567241, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7187081575393677, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 2695857}}
 0: {'global_steps': 1758, 'eval_loss': 1.3136340379714966, 'eval_mlm_accuracy': 0.7187081575393677}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134573477, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 23014.975435581357}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 2846403}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134573478, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2846403, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 2846403}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134573781, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2846403, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 2846403}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134573782, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7194647789001465, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 2846403}}
 0: {'global_steps': 1856, 'eval_loss': 1.3083146810531616, 'eval_mlm_accuracy': 0.7194647789001465}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134580558, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 21192.089182381158}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 2996480}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134580560, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2996480, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 2996480}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134580866, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2996480, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 2996480}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134580867, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7194671034812927, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 2996480}}
 0: {'global_steps': 1954, 'eval_loss': 1.3085068464279175, 'eval_mlm_accuracy': 0.7194671034812927}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134587049, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 22926.849308054065}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 3145290}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134587050, "event_type": "INTERVAL_START", "key": "eval_start", "value": 3145290, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 3145290}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134587354, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 3145290, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 3145290}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134587355, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7199294567108154, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 3145290}}
 0: {'global_steps': 2051, 'eval_loss': 1.3058960437774658, 'eval_mlm_accuracy': 0.7199294567108154}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134593606, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 22950.121156361947}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2019, "epoch_num": 3295776}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134593607, "event_type": "INTERVAL_START", "key": "eval_start", "value": 3295776, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2025, "epoch_num": 3295776}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134593913, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 3295776, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2034, "epoch_num": 3295776}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134593914, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.72047358751297, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2038, "epoch_num": 3295776}}
 0: {'global_steps': 2149, 'eval_loss': 1.3008912801742554, 'eval_mlm_accuracy': 0.72047358751297}
 0: 0.720474 > 0.720000, Target MLM Accuracy reached at 2149
 0: Training runs 2.5170326312383016 mins sustained_training_time 0
 0: (1, 2149.0) {'final_loss': 0.0}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134593915, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2157, "first_epoch_num": 1}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134593916, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2160, "epoch_num": 3295776}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134593917, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3295776, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2162}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134593918, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 10000, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2165}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134593920, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2168, "status": "success"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1746134593921, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 22413.789764828016, "epoch_num": 3295776}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2207, "step": [2, 2149]}}
 0: {'e2e_time': 163.73588705062866, 'training_sequences_per_second': 37428.19551532245, 'final_loss': 0.0, 'raw_train_time': 151.0219748020172}
 5: [rank5]:[W501 21:23:14.459509202 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
 9: [rank9]:[W501 21:23:17.384578937 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
 6: [rank6]:[W501 21:23:15.576252551 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
11: [rank11]:[W501 21:23:17.475644191 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
14: [rank14]:[W501 21:23:17.494989151 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
10: [rank10]:[W501 21:23:17.497026516 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
12: [rank12]:[W501 21:23:18.593862123 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
 4: [rank4]:[W501 21:23:15.745378781 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
 3: [rank3]:[W501 21:23:15.764577777 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
 2: [rank2]:[W501 21:23:15.777400655 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
13: [rank13]:[W501 21:23:18.644518140 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
 7: [rank7]:[W501 21:23:15.802267490 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
15: [rank15]:[W501 21:23:18.658774742 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
 1: [rank1]:[W501 21:23:15.815386212 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
 0: [rank0]:[W501 21:23:15.880964907 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
 8: [rank8]:[W501 21:23:18.914553963 communicator.cpp:111] Warning: the environment variable NVFUSER_MASTER_ADDR must be specified in multi-node environment (function parseEnv)
++ date +%s
+ echo 'RUNANDTIME_STOP 1746134600'
RUNANDTIME_STOP 1746134600
+ set -e
